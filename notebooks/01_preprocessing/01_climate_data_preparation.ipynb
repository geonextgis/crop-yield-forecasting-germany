{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "603bfe27",
   "metadata": {},
   "source": [
    "## Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b1fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import concurrent.futures\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from datetime import datetime, timedelta\n",
    "from glob import glob\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import weibull_min\n",
    "from shapely.geometry import Point\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "WORK_DIR = \"/beegfs/halder/GITHUB/RESEARCH/crop-yield-forecasting-germany/\"\n",
    "os.chdir(WORK_DIR)\n",
    "MAIN_DATA_DIR = \"/beegfs/halder/DATA/\"\n",
    "PHENO_DATA_DIR = \"/beegfs/halder/GITHUB/RESEARCH/land-surface-phenology/data/phenology_processed_(DE)_all\"\n",
    "DWD_DATA_DIR = (\n",
    "    \"/beegfs/common/data/climate/dwd/csvs/germany_ubn_1951-01-01_to_2024-08-30\"\n",
    ")\n",
    "OUT_DIR = os.path.join(WORK_DIR, \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fecfd1",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "CROP = \"winter_wheat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae8cdf",
   "metadata": {},
   "source": [
    "## Read the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5984f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the NUTS1 and NUTS3 shapefile for DE\n",
    "de_nuts1_gdf = gpd.read_file(os.path.join(MAIN_DATA_DIR, \"DE_NUTS\", \"DE_NUTS_3.shp\"))\n",
    "de_nuts1_gdf = de_nuts1_gdf[\n",
    "    de_nuts1_gdf[\"LEVL_CODE\"] == 1\n",
    "]  # filter only NUT1 level code\n",
    "de_nuts1_gdf.rename(\n",
    "    columns={\"NUTS_ID\": \"STATE_ID\", \"NUTS_NAME\": \"STATE_NAME\"}, inplace=True\n",
    ")\n",
    "\n",
    "de_nuts3_gdf = gpd.read_file(os.path.join(MAIN_DATA_DIR, \"DE_NUTS\", \"DE_NUTS_3.shp\"))\n",
    "de_nuts3_gdf = de_nuts3_gdf[\n",
    "    de_nuts3_gdf[\"LEVL_CODE\"] == 3\n",
    "]  # filter only NUT3 level code\n",
    "\n",
    "de_nuts1_gdf.to_crs(crs=\"EPSG:25832\", inplace=True)\n",
    "de_nuts3_gdf.to_crs(crs=\"EPSG:25832\", inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "de_nuts3_gdf.plot(\n",
    "    ax=ax,\n",
    "    column=\"NUTS_NAME\",\n",
    "    cmap=\"Set3\",\n",
    "    edgecolor=\"grey\",\n",
    "    linewidth=0.5,\n",
    "    label=\"NUTS3\",\n",
    ")\n",
    "de_nuts1_gdf.plot(ax=ax, facecolor=\"none\", edgecolor=\"k\", linewidth=1, label=\"NUTS1\")\n",
    "plt.show()\n",
    "\n",
    "print(de_nuts1_gdf.shape, de_nuts3_gdf.shape)\n",
    "de_nuts3_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7a1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the DE DWD grip file path\n",
    "DE_DWD_json_path = os.path.join(\n",
    "    MAIN_DATA_DIR, \"DE_DWD_Lat_Lon\", \"latlon_to_rowcol.json\"\n",
    ")\n",
    "\n",
    "with open(DE_DWD_json_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert data to GeoDataFrame\n",
    "records = []\n",
    "for coord, index in data:\n",
    "    lat, lon = coord\n",
    "    row, col = index\n",
    "    point = Point(lon, lat)\n",
    "    records.append({\"row\": row, \"col\": col, \"geometry\": point})\n",
    "\n",
    "latlon_gdf = gpd.GeoDataFrame(records, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "latlon_gdf.to_crs(crs=\"EPSG:25832\", inplace=True)\n",
    "\n",
    "latlon_gdf = gpd.sjoin(\n",
    "    left_df=latlon_gdf,\n",
    "    right_df=de_nuts3_gdf[[\"NUTS_ID\", \"NUTS_NAME\", \"geometry\"]],\n",
    "    how=\"inner\",\n",
    "    predicate=\"intersects\",\n",
    ").drop(columns=\"index_right\")\n",
    "\n",
    "latlon_gdf[\"rowcol\"] = list(zip(latlon_gdf[\"row\"], latlon_gdf[\"col\"]))\n",
    "\n",
    "print(latlon_gdf.shape)\n",
    "latlon_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bcc923",
   "metadata": {},
   "source": [
    "## Process the phenology data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cedfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_doy(data_path):\n",
    "    \"\"\"\n",
    "    Get the start DOY, end DOY, and full DOY window for sowing–harvest period.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : str\n",
    "        Path to a CSV file with columns 'sowing_date', 'flowering_date', 'maturity_date'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            'sow_month': int,             # Sow month\n",
    "            'harvest_month': int          # Harvest month\n",
    "            'sow_month_start_doy': int,   # DOY of the first sowing month (day 1 of month)\n",
    "            'harvest_month_end_doy': int, # DOY of the last harvest month (day 31 of month)\n",
    "            'window_length_in_days': int, # Total days in the sowing–harvest window\n",
    "            'window_doys': list[int]      # List of DOYs in the window\n",
    "        }\n",
    "    \"\"\"\n",
    "    REF_YEAR = 1970  # Non-leap reference year for DOY calculation\n",
    "\n",
    "    # Read phenology data\n",
    "    phenology_df = pd.read_csv(data_path)\n",
    "\n",
    "    # Get earliest sowing month and latest harvest month\n",
    "    sow_month = (\n",
    "        pd.to_datetime(phenology_df[\"sowing_date\"]).dt.month.median().astype(int)\n",
    "    )\n",
    "    harvest_month = (\n",
    "        pd.to_datetime(phenology_df[\"maturity_date\"]).dt.month.median().astype(int)\n",
    "    )\n",
    "    print(sow_month, harvest_month)\n",
    "    # Convert months to DOY\n",
    "    sow_doy = datetime(REF_YEAR, sow_month, 1).timetuple().tm_yday\n",
    "\n",
    "    try:\n",
    "        harvest_doy = datetime(REF_YEAR, harvest_month, 31).timetuple().tm_yday\n",
    "    except:\n",
    "        harvest_doy = datetime(REF_YEAR, harvest_month, 30).timetuple().tm_yday\n",
    "\n",
    "    # Build DOY window, handling year wrap-around\n",
    "    if harvest_doy < sow_doy:\n",
    "        window_doys = list(range(sow_doy, 366)) + list(range(1, harvest_doy + 1))\n",
    "    else:\n",
    "        window_doys = list(range(sow_doy, harvest_doy + 1))\n",
    "\n",
    "    return {\n",
    "        \"sow_month\": sow_month,\n",
    "        \"harvest_month\": harvest_month,\n",
    "        \"sow_month_start_doy\": sow_doy,\n",
    "        \"harvest_month_end_doy\": harvest_doy,\n",
    "        \"window_length_in_days\": len(window_doys),\n",
    "        \"window_doys\": window_doys,\n",
    "    }\n",
    "\n",
    "\n",
    "# Read the phenology data\n",
    "phenology_summary = get_start_end_doy(\n",
    "    os.path.join(\n",
    "        \"/beegfs/halder/GITHUB/RESEARCH/crop-yield-forecasting-germany/data/processed\",\n",
    "        CROP,\n",
    "        f\"{CROP}_phenology.csv\",\n",
    "    )\n",
    ")\n",
    "print(\"Sow Month:\", phenology_summary[\"sow_month\"])\n",
    "print(\"Harvest Month:\", phenology_summary[\"harvest_month\"])\n",
    "print(\"Sow Month Start DOY:\", phenology_summary[\"sow_month_start_doy\"])\n",
    "print(\"Harvest Month End DOY:\", phenology_summary[\"harvest_month_end_doy\"])\n",
    "print(\"Window Length in Days:\", phenology_summary[\"window_length_in_days\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58441ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the phenology data\n",
    "phenology_df = pd.read_csv(\n",
    "    os.path.join(\n",
    "        \"/beegfs/halder/GITHUB/RESEARCH/crop-yield-forecasting-germany/data/processed\",\n",
    "        CROP,\n",
    "        f\"{CROP}_phenology.csv\",\n",
    "    )\n",
    ")\n",
    "phenology_df = phenology_df[phenology_df[\"harvest_year\"] >= 2000]\n",
    "\n",
    "for date_col in [\"sowing_date\", \"flowering_date\", \"maturity_date\"]:\n",
    "    event = date_col.split(\"_\")[0]\n",
    "    phenology_df[date_col] = pd.to_datetime(phenology_df[date_col], format=\"%Y-%m-%d\")\n",
    "    phenology_df[f\"{event}_DOY\"] = phenology_df[date_col].dt.dayofyear\n",
    "\n",
    "sow_year = phenology_df[\"sowing_date\"].dt.year\n",
    "start_month = phenology_summary[\"sow_month\"] - 3  # starting 3 months advance\n",
    "start_date = pd.to_datetime(\n",
    "    sow_year.astype(str) + \"-\" + start_month.astype(str) + \"-01\"\n",
    ")\n",
    "phenology_df[\"start_date\"] = start_date\n",
    "\n",
    "harvest_year = phenology_df[\"maturity_date\"].dt.year\n",
    "end_month = phenology_summary[\"harvest_month\"]\n",
    "end_date = pd.to_datetime(\n",
    "    harvest_year.astype(str) + \"-\" + end_month.astype(str) + \"-01\"\n",
    ") + pd.offsets.MonthEnd(0)\n",
    "phenology_df[\"end_date\"] = end_date\n",
    "\n",
    "\n",
    "def extend_phenology(df, target_year=2024):\n",
    "    extended_rows = []\n",
    "\n",
    "    # Define global year range so ALL NUTS get the same years\n",
    "    global_min_year = 2000\n",
    "    global_max_year = target_year\n",
    "    all_years = set(range(global_min_year, global_max_year + 1))\n",
    "\n",
    "    # Loop through each NUTS independently\n",
    "    for nuts_id, group in df.groupby(\"NUTS_ID\"):\n",
    "        years_present = set(group[\"harvest_year\"])\n",
    "        missing_years = sorted(all_years - years_present)\n",
    "\n",
    "        for year in missing_years:\n",
    "            # Find the closest previous year available as template\n",
    "            template_years = group[group[\"harvest_year\"] <= year]\n",
    "            if template_years.empty:\n",
    "                # If no previous year, use the earliest available year\n",
    "                template_years = group[group[\"harvest_year\"] > year]\n",
    "                if template_years.empty:\n",
    "                    continue\n",
    "                template_row = template_years.iloc[0]\n",
    "            else:\n",
    "                template_row = template_years.iloc[-1]\n",
    "\n",
    "            new_row = template_row.copy()\n",
    "            new_row[\"harvest_year\"] = year\n",
    "\n",
    "            def shift_date(date_str, offset):\n",
    "                d = pd.to_datetime(date_str)\n",
    "                return (d + pd.DateOffset(years=offset)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "            offset = year - template_row[\"harvest_year\"]\n",
    "            new_row[\"sowing_date\"] = shift_date(template_row[\"sowing_date\"], offset)\n",
    "            new_row[\"emergence_date\"] = shift_date(\n",
    "                template_row[\"emergence_date\"], offset\n",
    "            )\n",
    "            new_row[\"flowering_date\"] = shift_date(\n",
    "                template_row[\"flowering_date\"], offset\n",
    "            )\n",
    "            new_row[\"maturity_date\"] = shift_date(template_row[\"maturity_date\"], offset)\n",
    "            new_row[\"start_date\"] = shift_date(template_row[\"start_date\"], offset)\n",
    "            new_row[\"end_date\"] = shift_date(template_row[\"end_date\"], offset)\n",
    "\n",
    "            extended_rows.append(new_row)\n",
    "\n",
    "    extended_df = pd.concat([df, pd.DataFrame(extended_rows)], ignore_index=True)\n",
    "    extended_df = extended_df.sort_values(by=[\"NUTS_ID\", \"harvest_year\"]).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    return extended_df\n",
    "\n",
    "\n",
    "def fill_missing_nuts(phenology_df, all_nuts_gdf, target_year=2024):\n",
    "    \"\"\"\n",
    "    For NUTS IDs present in the shapefile but missing from phenology_df,\n",
    "    find the spatially closest NUTS that HAS phenology data and copy its rows.\n",
    "    \"\"\"\n",
    "    nuts_with_pheno = set(phenology_df[\"NUTS_ID\"].unique())\n",
    "    all_nuts_ids = set(all_nuts_gdf[\"NUTS_ID\"].unique())\n",
    "    missing_nuts = all_nuts_ids - nuts_with_pheno\n",
    "\n",
    "    print(f\"NUTS with phenology: {len(nuts_with_pheno)}\")\n",
    "    print(f\"NUTS missing phenology: {len(missing_nuts)}\")\n",
    "\n",
    "    if not missing_nuts:\n",
    "        return phenology_df\n",
    "\n",
    "    # Compute centroids for distance calculation\n",
    "    nuts_centroids = all_nuts_gdf.copy()\n",
    "    nuts_centroids[\"centroid\"] = nuts_centroids.geometry.centroid\n",
    "\n",
    "    extra_rows = []\n",
    "\n",
    "    for nuts_id in missing_nuts:\n",
    "        # Get centroid of the missing NUTS\n",
    "        missing_geom = nuts_centroids.loc[\n",
    "            nuts_centroids[\"NUTS_ID\"] == nuts_id, \"centroid\"\n",
    "        ].values[0]\n",
    "\n",
    "        # Compute distances to all NUTS that have phenology data\n",
    "        candidate_gdf = nuts_centroids[\n",
    "            nuts_centroids[\"NUTS_ID\"].isin(nuts_with_pheno)\n",
    "        ].copy()\n",
    "        candidate_gdf[\"dist\"] = candidate_gdf[\"centroid\"].apply(\n",
    "            lambda c: missing_geom.distance(c)\n",
    "        )\n",
    "        closest_nuts_id = candidate_gdf.loc[candidate_gdf[\"dist\"].idxmin(), \"NUTS_ID\"]\n",
    "\n",
    "        # Copy all rows from closest NUTS, replace NUTS_ID\n",
    "        closest_rows = phenology_df[phenology_df[\"NUTS_ID\"] == closest_nuts_id].copy()\n",
    "        closest_rows[\"NUTS_ID\"] = nuts_id\n",
    "        extra_rows.append(closest_rows)\n",
    "        print(f\"  {nuts_id} <- copied from {closest_nuts_id}\")\n",
    "\n",
    "    filled_df = pd.concat([phenology_df] + extra_rows, ignore_index=True)\n",
    "    filled_df = filled_df.sort_values(by=[\"NUTS_ID\", \"harvest_year\"]).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    return filled_df\n",
    "\n",
    "\n",
    "phenology_df = extend_phenology(phenology_df, target_year=2024)\n",
    "phenology_df = fill_missing_nuts(phenology_df, de_nuts3_gdf, target_year=2024)\n",
    "\n",
    "# Verify: all NUTS should now have the same number of years\n",
    "year_counts = phenology_df.groupby(\"NUTS_ID\")[\"harvest_year\"].count()\n",
    "print(f\"\\nYear counts — min: {year_counts.min()}, max: {year_counts.max()}\")\n",
    "print(f\"Total unique NUTS: {phenology_df['NUTS_ID'].nunique()}\")\n",
    "\n",
    "date_cols = [\n",
    "    \"sowing_date\",\n",
    "    \"emergence_date\",\n",
    "    \"flowering_date\",\n",
    "    \"maturity_date\",\n",
    "    \"start_date\",\n",
    "    \"end_date\",\n",
    "]\n",
    "for col in date_cols:\n",
    "    if col in phenology_df.columns:\n",
    "        phenology_df[col] = pd.to_datetime(phenology_df[col]).dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "phenology_df.to_csv(\n",
    "    os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"data\",\n",
    "        \"processed\",\n",
    "        CROP,\n",
    "        f\"{CROP}_phenology_final.csv\",\n",
    "    ),\n",
    "    index=False,\n",
    ")\n",
    "print(phenology_df.shape)\n",
    "phenology_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a065d092",
   "metadata": {},
   "source": [
    "## Extract climate data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9ec11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phenology(nuts_id, year):\n",
    "    subset = phenology_df[\n",
    "        (phenology_df[\"NUTS_ID\"] == nuts_id) & (phenology_df[\"harvest_year\"] == year)\n",
    "    ]\n",
    "    if subset.empty:\n",
    "        return None\n",
    "    return subset.iloc[0].to_dict()\n",
    "\n",
    "\n",
    "def extract_climate_data_by_nuts(nuts_id, out_dir):\n",
    "    row_cols = np.array(latlon_gdf.loc[latlon_gdf[\"NUTS_ID\"] == nuts_id, \"rowcol\"])\n",
    "\n",
    "    file_paths = []\n",
    "    for row, col in row_cols:\n",
    "        fpath = os.path.join(\n",
    "            DWD_DATA_DIR, str(row), f\"daily_mean_RES1_C{col}R{row}.csv.gz\"\n",
    "        )\n",
    "        if os.path.exists(fpath):\n",
    "            file_paths.append(fpath)\n",
    "\n",
    "    if not file_paths:\n",
    "        print(f\"No climate files found for hex ID {nuts_id}\")\n",
    "        return\n",
    "\n",
    "    column_names = {\n",
    "        \"Date\": \"date\",\n",
    "        \"TempMin\": \"tmin\",\n",
    "        \"TempMax\": \"tmax\",\n",
    "        \"Precipitation\": \"prec\",\n",
    "        \"Radiation\": \"rad\",\n",
    "        \"SunshineDuration\": \"sun_dur\",\n",
    "        \"SoilMoisture\": \"soil_moist\",\n",
    "        \"SoilTemperature\": \"soil_temp\",\n",
    "        \"RefETcalc\": \"et0\",\n",
    "        \"RefETdwd\": \"et0_dwd\",\n",
    "        \"RelHumCalc\": \"rh\",\n",
    "    }\n",
    "\n",
    "    clim_dfs = []\n",
    "    for f in file_paths:\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                f,\n",
    "                delimiter=\"\\t\",\n",
    "                usecols=[\n",
    "                    \"Date\",\n",
    "                    \"Precipitation\",\n",
    "                    \"TempMin\",\n",
    "                    \"TempMax\",\n",
    "                    \"Radiation\",\n",
    "                    \"SunshineDuration\",\n",
    "                    \"SoilMoisture\",\n",
    "                    \"SoilTemperature\",\n",
    "                    \"Windspeed\",\n",
    "                    \"RefETcalc\",\n",
    "                    \"RefETdwd\",\n",
    "                    \"RelHumCalc\",\n",
    "                ],\n",
    "            )\n",
    "            df.rename(columns=column_names, inplace=True)\n",
    "            df = df[list(column_names.values())]\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "            # Saturation vapor pressure (kPa)\n",
    "            df[\"es_tmin\"] = 0.6108 * np.exp((17.27 * df[\"tmin\"]) / (df[\"tmin\"] + 237.3))\n",
    "            df[\"es_tmax\"] = 0.6108 * np.exp((17.27 * df[\"tmax\"]) / (df[\"tmax\"] + 237.3))\n",
    "            df[\"es\"] = (df[\"es_tmin\"] + df[\"es_tmax\"]) / 2\n",
    "            df[\"ea\"] = (df[\"rh\"] / 100) * df[\"es\"]\n",
    "            df[\"vpd\"] = df[\"es\"] - df[\"ea\"]\n",
    "\n",
    "            # Average temperature and water balance\n",
    "            df[\"tavg\"] = (df[\"tmax\"] + df[\"tmin\"]) / 2\n",
    "            df[\"cwb\"] = df[\"prec\"] - df[\"et0\"]\n",
    "            df[\"rad\"] = df[\"rad\"] / 1000  # kJ/m²·day to MJ/m²·day\n",
    "\n",
    "            clim_dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {f}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Merge and aggregate\n",
    "    clim_merged = pd.concat(clim_dfs).groupby(\"date\").mean().reset_index()\n",
    "    clim_merged = clim_merged[\n",
    "        [\n",
    "            \"date\",\n",
    "            \"tmin\",\n",
    "            \"tmax\",\n",
    "            \"tavg\",\n",
    "            \"prec\",\n",
    "            \"rad\",\n",
    "            \"sun_dur\",\n",
    "            \"soil_moist\",\n",
    "            \"soil_temp\",\n",
    "            \"et0\",\n",
    "            \"vpd\",\n",
    "            \"cwb\",\n",
    "        ]\n",
    "    ]\n",
    "    clim_merged.iloc[:, 1:] = clim_merged.iloc[:, 1:].round(3)\n",
    "\n",
    "    # Loop over years using phenology windows\n",
    "    for year in sorted(phenology_df[\"harvest_year\"].unique()):\n",
    "        pheno = get_phenology(nuts_id, year)\n",
    "        if pheno is None:\n",
    "            continue\n",
    "\n",
    "        start_date = pheno[\"start_date\"]\n",
    "        end_date = pheno[\"end_date\"]\n",
    "\n",
    "        clim_year = clim_merged[\n",
    "            (clim_merged[\"date\"] >= start_date) & (clim_merged[\"date\"] <= end_date)\n",
    "        ].copy()\n",
    "\n",
    "        if clim_year.empty:\n",
    "            continue\n",
    "\n",
    "        save_path = os.path.join(out_dir, f\"{nuts_id}_{year}.csv\")\n",
    "        clim_year.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR_CLIMATE = os.path.join(WORK_DIR, \"data\", \"interim\", \"climate\", f\"{CROP}\")\n",
    "\n",
    "if os.path.exists(OUT_DIR_CLIMATE):\n",
    "    print(\"Directory already exists!\")\n",
    "else:\n",
    "    os.makedirs(OUT_DIR_CLIMATE, exist_ok=True)\n",
    "    print(\"Directory successfully created!\")\n",
    "\n",
    "# Prepare all hex_ids to be processed\n",
    "nuts_ids = latlon_gdf[\"NUTS_ID\"].unique()\n",
    "\n",
    "\n",
    "# Function to wrap\n",
    "def wrapper(nuts_id):\n",
    "    extract_climate_data_by_nuts(nuts_id, OUT_DIR_CLIMATE)\n",
    "\n",
    "\n",
    "# Use tqdm's process_map instead of executor.map\n",
    "process_map(wrapper, nuts_ids, max_workers=10, chunksize=1)\n",
    "print(\"Climate data computation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
