{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48f366cf",
   "metadata": {},
   "source": [
    "## Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6505117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"DeJavu Serif\"\n",
    "plt.rcParams[\"font.serif\"] = \"Times New Roman\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "WORK_DIR = \"/beegfs/halder/GITHUB/RESEARCH/crop-yield-forecasting-germany/\"\n",
    "os.chdir(WORK_DIR)\n",
    "MAIN_DATA_DIR = \"/beegfs/halder/DATA/\"\n",
    "OUT_DIR = os.path.join(WORK_DIR, \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CROP = \"silage_maize\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abda9c84",
   "metadata": {},
   "source": [
    "## Read the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the NUTS1 and NUTS3 shapefile for DE\n",
    "de_nuts1_gdf = gpd.read_file(os.path.join(MAIN_DATA_DIR, \"DE_NUTS\", \"DE_NUTS_3.shp\"))\n",
    "de_nuts1_gdf = de_nuts1_gdf[\n",
    "    de_nuts1_gdf[\"LEVL_CODE\"] == 1\n",
    "]  # filter only NUT1 level code\n",
    "de_nuts1_gdf.rename(\n",
    "    columns={\"NUTS_ID\": \"STATE_ID\", \"NUTS_NAME\": \"STATE_NAME\"}, inplace=True\n",
    ")\n",
    "\n",
    "de_nuts3_gdf = gpd.read_file(os.path.join(MAIN_DATA_DIR, \"DE_NUTS\", \"DE_NUTS_3.shp\"))\n",
    "de_nuts3_gdf = de_nuts3_gdf[\n",
    "    de_nuts3_gdf[\"LEVL_CODE\"] == 3\n",
    "]  # filter only NUT3 level code\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "de_nuts3_gdf.plot(\n",
    "    ax=ax,\n",
    "    column=\"NUTS_NAME\",\n",
    "    cmap=\"Set3\",\n",
    "    edgecolor=\"grey\",\n",
    "    linewidth=0.5,\n",
    "    label=\"NUTS3\",\n",
    ")\n",
    "de_nuts1_gdf.plot(ax=ax, facecolor=\"none\", edgecolor=\"k\", linewidth=1, label=\"NUTS1\")\n",
    "plt.show()\n",
    "\n",
    "print(de_nuts1_gdf.shape, de_nuts3_gdf.shape)\n",
    "de_nuts3_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaaa61f",
   "metadata": {},
   "source": [
    "## Extract the remote sensing variables (NDVI, EVI, fPAR, and LAI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2235064",
   "metadata": {},
   "source": [
    "### Initialize and setup Earth Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a4b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "from geeagri.extract import TimeseriesExtractor\n",
    "\n",
    "# ee.Authenticate()\n",
    "ee.Initialize(project=\"ee-geonextgis\")\n",
    "\n",
    "# Instantiate a Map object\n",
    "Map = geemap.Map(basemap=\"SATELLITE\")\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8a81a",
   "metadata": {},
   "source": [
    "### Extract NDVI and EVI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f3126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the crop mask\n",
    "crop_dict = {\n",
    "    \"winter_wheat\": 1110,\n",
    "    \"winter_barley\": 1120,\n",
    "    \"silage_maize\": 1130,\n",
    "    \"winter_rapeseed\": 1430,\n",
    "    \"winter_rye\": 1150,\n",
    "}\n",
    "\n",
    "de_nuts3_ee = ee.FeatureCollection(\"projects/ee-geonextgis/assets/DE/DE_NUTS_3\")\n",
    "\n",
    "crop_type_2017 = (\n",
    "    ee.Image(f\"projects/ee-geonextgis/assets/DE/DE_Crop_Type_2017\")\n",
    "    .select(0)\n",
    "    .eq(crop_dict[CROP])\n",
    "    .selfMask()\n",
    ")\n",
    "crop_type_2018 = (\n",
    "    ee.Image(f\"projects/ee-geonextgis/assets/DE/DE_Crop_Type_2018\")\n",
    "    .select(0)\n",
    "    .eq(crop_dict[CROP])\n",
    "    .selfMask()\n",
    ")\n",
    "crop_type_2019 = (\n",
    "    ee.Image(f\"projects/ee-geonextgis/assets/DE/DE_Crop_Type_2019\")\n",
    "    .select(0)\n",
    "    .eq(crop_dict[CROP])\n",
    "    .selfMask()\n",
    ")\n",
    "crop_type_2020 = (\n",
    "    ee.Image(f\"projects/ee-geonextgis/assets/DE/DE_Crop_Type_2020\")\n",
    "    .select(0)\n",
    "    .eq(crop_dict[CROP])\n",
    "    .selfMask()\n",
    ")\n",
    "crop_type_2021 = (\n",
    "    ee.Image(f\"projects/ee-geonextgis/assets/DE/DE_Crop_Type_2021\")\n",
    "    .select(0)\n",
    "    .eq(crop_dict[CROP])\n",
    "    .selfMask()\n",
    ")\n",
    "\n",
    "crop_mask = (\n",
    "    ee.ImageCollection(\n",
    "        [crop_type_2017, crop_type_2018, crop_type_2019, crop_type_2020, crop_type_2021]\n",
    "    )\n",
    "    .reduce(ee.Reducer.max())\n",
    "    .selfMask()\n",
    ")\n",
    "\n",
    "Map.addLayer(crop_mask, {\"min\": 0, \"max\": 1, \"palette\": \"yellow\"}, \"Cropmask\")\n",
    "Map.centerObject(de_nuts3_ee.geometry().bounds(), 6)\n",
    "\n",
    "\n",
    "def applyBitmask(img):\n",
    "    # Extract the QA band\n",
    "    qa = img.select(\"SummaryQA\")\n",
    "\n",
    "    # Create a mask for bits 0-1 == 00 (good quality)\n",
    "    goodQualityMask = qa.bitwiseAnd(3).eq(0)\n",
    "\n",
    "    return img.updateMask(goodQualityMask).copyProperties(img, [\"system:time_start\"])\n",
    "\n",
    "\n",
    "# Load the MOD13Q1.061 Terra Vegetation Indices 16-Day Global 250m data\n",
    "modis = (\n",
    "    ee.ImageCollection(\"MODIS/061/MOD13Q1\")\n",
    "    .select([\"NDVI\", \"EVI\", \"SummaryQA\"])\n",
    "    .map(\n",
    "        lambda img: img.updateMask(crop_mask).copyProperties(img, [\"system:time_start\"])\n",
    "    )\n",
    "    .map(lambda img: applyBitmask(img))\n",
    "    .select([\"NDVI\", \"EVI\"])\n",
    ")\n",
    "\n",
    "output_dir_ndvi = os.path.join(\n",
    "    WORK_DIR, \"data\", \"interim\", \"remote_sensing\", CROP, \"ndvi_evi\"\n",
    ")\n",
    "\n",
    "if os.path.exists(output_dir_ndvi):\n",
    "    print(\"Directory already exists!\")\n",
    "else:\n",
    "    os.makedirs(output_dir_ndvi, exist_ok=True)\n",
    "    print(\"Directory successfully created!\")\n",
    "\n",
    "de_nuts3_gdf_reprojected = de_nuts3_gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Extract timeseries in parallel for all samples\n",
    "ts_extractor = TimeseriesExtractor(\n",
    "    image_collection=modis,\n",
    "    sample_gdf=de_nuts3_gdf_reprojected,\n",
    "    identifier=\"NUTS_ID\",\n",
    "    out_dir=output_dir_ndvi,\n",
    "    selectors=[\"NDVI\", \"EVI\"],\n",
    "    scale=250,\n",
    "    num_processes=20,  # parallel processes\n",
    "    start_date=\"2000-01-01\",\n",
    "    end_date=\"2025-08-31\",\n",
    "    reducer=\"MEAN\",\n",
    ")\n",
    "\n",
    "# Run extraction\n",
    "# ts_extractor.extract_timeseries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35790b",
   "metadata": {},
   "source": [
    "### Extract fPAR and LAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7cedab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to mask low-quality pixels\n",
    "def applyBitmask(img):\n",
    "    qc = img.select(\"FparLai_QC\")\n",
    "\n",
    "    # Bit 0: MODLAND_QC bits ‚Äî keep both 0 and 1, so no filtering\n",
    "    modland = ee.Image(1)  # always true mask\n",
    "\n",
    "    # Bits 3‚Äì4: CloudState (00 = clear)\n",
    "    cloud_state = qc.rightShift(3).bitwiseAnd(3).eq(0)\n",
    "\n",
    "    # Bits 5‚Äì7: SCF_QC (000 or 001 = good or best quality)\n",
    "    scf = qc.rightShift(5).bitwiseAnd(7)\n",
    "    scf_mask = scf.lte(1)\n",
    "\n",
    "    # Combine masks\n",
    "    mask = modland.And(cloud_state).And(scf_mask)\n",
    "\n",
    "    # Apply mask to keep only good pixels\n",
    "    return (\n",
    "        img.updateMask(mask)\n",
    "        .select([\"Fpar_500m\", \"Lai_500m\"])\n",
    "        .copyProperties(img, img.propertyNames())\n",
    "    )\n",
    "\n",
    "\n",
    "# Load the MOD15A2H.061: Terra Leaf Area Index/FPAR 8-Day Global 500m data\n",
    "modis = (\n",
    "    ee.ImageCollection(\"MODIS/061/MOD15A2H\")\n",
    "    .select([\"Fpar_500m\", \"Lai_500m\", \"FparLai_QC\"])\n",
    "    .map(\n",
    "        lambda img: img.updateMask(crop_mask).copyProperties(img, [\"system:time_start\"])\n",
    "    )\n",
    "    .map(applyBitmask)\n",
    ")\n",
    "\n",
    "output_dir_lai = os.path.join(\n",
    "    WORK_DIR, \"data\", \"interim\", \"remote_sensing\", CROP, \"lai_fpar\"\n",
    ")\n",
    "if os.path.exists(output_dir_lai):\n",
    "    print(\"Directory already exists!\")\n",
    "else:\n",
    "    os.makedirs(output_dir_lai, exist_ok=True)\n",
    "    print(\"Directory successfully created!\")\n",
    "\n",
    "\n",
    "# Extract timeseries in parallel for all samples\n",
    "ts_extractor = TimeseriesExtractor(\n",
    "    image_collection=modis,\n",
    "    sample_gdf=de_nuts3_gdf_reprojected,\n",
    "    identifier=\"NUTS_ID\",\n",
    "    out_dir=output_dir_lai,\n",
    "    selectors=[\"Fpar_500m\", \"Lai_500m\"],\n",
    "    scale=500,\n",
    "    num_processes=20,  # parallel processes\n",
    "    start_date=\"2000-01-01\",\n",
    "    end_date=\"2025-08-31\",\n",
    "    reducer=\"MEAN\",\n",
    ")\n",
    "\n",
    "# Run extraction\n",
    "# ts_extractor.extract_timeseries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b7b2a",
   "metadata": {},
   "source": [
    "## Post process and combine all the remote sensing variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50abe762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# Store the file paths\n",
    "ndvi_file_paths, fpar_file_paths = sorted(os.listdir(output_dir_ndvi)), sorted(\n",
    "    os.listdir(output_dir_lai)\n",
    ")\n",
    "\n",
    "print(\"Number of NDVI files:\", len(ndvi_file_paths))\n",
    "print(\"Number of FPAR files:\", len(fpar_file_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb4a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to post-process ndvi and evi data\n",
    "def process_ndvi(nuts_id):\n",
    "\n",
    "    index = nuts_id\n",
    "    file_path = os.path.join(output_dir_ndvi, f\"{index}.csv\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert to datetime and scale the values\n",
    "    df[\"date\"] = pd.to_datetime(df[\"time\"])\n",
    "    df[[\"ndvi\", \"evi\"]] = df[[\"NDVI\", \"EVI\"]] * 0.0001\n",
    "\n",
    "    # Keep only the columns we need\n",
    "    df = df[[\"date\", \"ndvi\", \"evi\"]]\n",
    "\n",
    "    # Apply the Savitzky-Golay filter\n",
    "    df[\"smoothed_ndvi\"] = savgol_filter(df[\"ndvi\"], window_length=5, polyorder=2)\n",
    "    df[\"smoothed_evi\"] = savgol_filter(df[\"evi\"], window_length=5, polyorder=2)\n",
    "\n",
    "    # Set the 'date' column as the index\n",
    "    df = df.set_index(\"date\")\n",
    "\n",
    "    # Create a new daily date range\n",
    "    start_date = df.index.min()\n",
    "    end_date = df.index.max()\n",
    "    daily_index = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "    # We only want to interpolate the smoothed values\n",
    "    df_smooth = df[[\"smoothed_ndvi\", \"smoothed_evi\"]]\n",
    "\n",
    "    # Reindex the smooth DataFrame to the new daily index.\n",
    "    # This creates rows for every day, with NaNs for the new days.\n",
    "    df_daily = df_smooth.reindex(daily_index)\n",
    "\n",
    "    # Interpolate to fill in the daily values\n",
    "    # 'cubic' creates a smooth, continuous curve.\n",
    "    df_daily_interpolated = df_daily.interpolate(method=\"cubic\")\n",
    "\n",
    "    # Fill any NaNs at the very start or end\n",
    "    df_daily_interpolated = df_daily_interpolated.bfill().ffill()\n",
    "    df_daily_interpolated.reset_index(inplace=True)\n",
    "\n",
    "    df_daily_interpolated.columns = [\"date\", \"ndvi\", \"evi\"]\n",
    "    df_daily_interpolated[[\"ndvi\", \"evi\"]] = df_daily_interpolated[\n",
    "        [\"ndvi\", \"evi\"]\n",
    "    ].round(3)\n",
    "\n",
    "    return df_daily_interpolated\n",
    "\n",
    "\n",
    "# Function to post-process fpar and lai data\n",
    "def process_fpar(nuts_id):\n",
    "\n",
    "    index = nuts_id\n",
    "    file_path = os.path.join(output_dir_lai, f\"{index}.csv\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert to datetime and scale the values\n",
    "    df[\"date\"] = pd.to_datetime(df[\"time\"])\n",
    "    df[[\"fpar\", \"lai\"]] = df[[\"Fpar_500m\", \"Lai_500m\"]]\n",
    "    df[\"fpar\"] = df[\"fpar\"] * 0.01\n",
    "    df[\"lai\"] = df[\"lai\"] * 0.1\n",
    "\n",
    "    # Keep only the columns we need\n",
    "    df = df[[\"date\", \"fpar\", \"lai\"]]\n",
    "\n",
    "    # Apply the Savitzky-Golay filter\n",
    "    df[\"smoothed_fpar\"] = savgol_filter(df[\"fpar\"], window_length=5, polyorder=2)\n",
    "    df[\"smoothed_lai\"] = savgol_filter(df[\"lai\"], window_length=5, polyorder=2)\n",
    "\n",
    "    # Set the 'date' column as the index\n",
    "    df = df.set_index(\"date\")\n",
    "\n",
    "    # Create a new daily date range\n",
    "    start_date = df.index.min()\n",
    "    end_date = df.index.max()\n",
    "    daily_index = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "    # We only want to interpolate the smoothed values\n",
    "    df_smooth = df[[\"smoothed_fpar\", \"smoothed_lai\"]]\n",
    "\n",
    "    # Reindex the smooth DataFrame to the new daily index.\n",
    "    # This creates rows for every day, with NaNs for the new days.\n",
    "    df_daily = df_smooth.reindex(daily_index)\n",
    "\n",
    "    # Interpolate to fill in the daily values\n",
    "    # 'cubic' creates a smooth, continuous curve.\n",
    "    df_daily_interpolated = df_daily.interpolate(method=\"cubic\")\n",
    "\n",
    "    # Fill any NaNs at the very start or end\n",
    "    df_daily_interpolated = df_daily_interpolated.bfill().ffill()\n",
    "    df_daily_interpolated.reset_index(inplace=True)\n",
    "\n",
    "    df_daily_interpolated.columns = [\"date\", \"fpar\", \"lai\"]\n",
    "    df_daily_interpolated[[\"fpar\", \"lai\"]] = df_daily_interpolated[\n",
    "        [\"fpar\", \"lai\"]\n",
    "    ].round(3)\n",
    "\n",
    "    return df_daily_interpolated\n",
    "\n",
    "\n",
    "# Function to compile remote sensing data\n",
    "def process_remote_sensing(nuts_id, out_dir=None):\n",
    "    ndvi_df = process_ndvi(nuts_id)\n",
    "    fpar_df = process_fpar(nuts_id)\n",
    "\n",
    "    merged_df = pd.merge(ndvi_df, fpar_df, on=\"date\", how=\"inner\")\n",
    "    file_name = f\"{nuts_id}.csv\"\n",
    "\n",
    "    # Skip grids with any missing values\n",
    "    if merged_df.isna().values.any():\n",
    "        return None\n",
    "\n",
    "    # Save if output directory provided\n",
    "    if out_dir:\n",
    "        merged_df.to_csv(os.path.join(out_dir, file_name), index=False)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# Function to compile remote sensing data parallely\n",
    "def run_parallel(nuts_ids, out_dir, max_workers=8):\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_remote_sensing, nuts_id, out_dir): nuts_id\n",
    "            for nuts_id in nuts_ids\n",
    "        }\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            nuts_id = futures[future]\n",
    "            try:\n",
    "                future.result()\n",
    "                print(f\"‚úÖ Finished {nuts_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error processing {nuts_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369f0703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-process the remote sensing files\n",
    "out_dir = os.path.join(WORK_DIR, \"data\", \"interim\", \"remote_sensing\", CROP, \"combined\")\n",
    "if os.path.exists(out_dir):\n",
    "    print(\"Directory already exists!\")\n",
    "else:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(\"Directory successfully created!\")\n",
    "\n",
    "run_parallel(nuts_ids=de_nuts3_gdf[\"NUTS_ID\"].unique(), out_dir=out_dir, max_workers=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec560831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "# Replace the error NUTS with the nearest one\n",
    "def find_nearest_nuts(nuts_id, nuts_gdf):\n",
    "    g = nuts_gdf.to_crs(\"EPSG:3857\").copy()\n",
    "    g[\"centroid\"] = g.geometry.centroid\n",
    "    target = g.loc[g[\"NUTS_ID\"] == nuts_id, \"centroid\"]\n",
    "\n",
    "    existed_nuts = [name.replace(\".csv\", \"\") for name in os.listdir(out_dir)]\n",
    "    g = g[g[\"NUTS_ID\"].isin(existed_nuts)]\n",
    "\n",
    "    if target.empty:\n",
    "        return None\n",
    "    target_geom = target.values[0]\n",
    "    g = g[g[\"NUTS_ID\"] != nuts_id].copy()\n",
    "    if g.empty:\n",
    "        return None\n",
    "    g[\"dist\"] = g[\"centroid\"].distance(target_geom)\n",
    "    nearest = g.sort_values(\"dist\").iloc[0][\"NUTS_ID\"]\n",
    "    return nearest\n",
    "\n",
    "\n",
    "existed_nuts = [name.replace(\".csv\", \"\") for name in os.listdir(out_dir)]\n",
    "\n",
    "for nuts_id in de_nuts3_gdf[\"NUTS_ID\"].unique():\n",
    "    if nuts_id not in existed_nuts:\n",
    "        nearest = find_nearest_nuts(nuts_id, de_nuts3_gdf)\n",
    "        if nearest is None:\n",
    "            print(f\"‚ùå No nearest NUTS found for {nuts_id}; skipping.\")\n",
    "            continue\n",
    "        src = os.path.join(out_dir, f\"{nearest}.csv\")\n",
    "        dst = os.path.join(out_dir, f\"{nuts_id}.csv\")\n",
    "        if os.path.exists(src):\n",
    "            try:\n",
    "                shutil.copy(src, dst)\n",
    "                print(f\"üîÅ Replaced {nuts_id} with nearest NUTS file {nearest}.csv\")\n",
    "            except Exception as copy_err:\n",
    "                print(f\"‚ùå Failed to copy fallback file for {nuts_id}: {copy_err}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"‚ùå Nearest file {nearest}.csv not available to copy for {nuts_id}; skipping.\"\n",
    "            )\n",
    "\n",
    "print(\"Number of final files:\", len(os.listdir(out_dir)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
