{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2df396",
   "metadata": {},
   "source": [
    "## Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4925608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "WORK_DIR = \"/beegfs/halder/GITHUB/RESEARCH/crop-yield-forecasting-germany/\"\n",
    "os.chdir(WORK_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f271488e",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "CROP = \"silage_maize\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422967ba",
   "metadata": {},
   "source": [
    "## Define the paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae8e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(WORK_DIR, \"data\", \"interim\")\n",
    "climate_dir = os.path.join(data_dir, \"climate\", CROP)\n",
    "rs_dir = os.path.join(data_dir, \"remote_sensing\", CROP, \"combined\")\n",
    "out_dir = os.path.join(WORK_DIR, \"data\", \"processed\", CROP, \"timeseries\")\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa7e686",
   "metadata": {},
   "source": [
    "## Load the valid indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fadcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the valid indices\n",
    "split_df = pd.read_csv(\n",
    "    os.path.join(WORK_DIR, \"data\", \"processed\", CROP, \"train_test_val_split.csv\")\n",
    ")\n",
    "\n",
    "valid_indices = (\n",
    "    split_df.groupby([\"NUTS_ID\", \"year\"])\n",
    "    .apply(lambda x: x.to_dict(\"records\"))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(\"Number of valid indices:\", len(valid_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724c201",
   "metadata": {},
   "source": [
    "## Process the timeseries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ecbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_grouped_files(nuts_id, year, climate_dir, rs_dir, out_dir):\n",
    "    \"\"\"\n",
    "    Process all grid-level climate CSVs belonging to a given NUTS_ID and year.\n",
    "    For each grid_id:\n",
    "        - Read climate CSV (grid-year)\n",
    "        - Read RS CSV (grid only)\n",
    "        - Merge them on 'date'\n",
    "    Concatenate all merged grids into one dataframe and save as parquet.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        # ---- Build input paths ----\n",
    "        c_path = os.path.join(climate_dir, f\"{nuts_id}_{year}.csv\")\n",
    "        r_path = os.path.join(rs_dir, f\"{nuts_id}.csv\")\n",
    "\n",
    "        if not os.path.exists(c_path):\n",
    "            return f\"Missing climate file: {c_path}\"\n",
    "\n",
    "        if not os.path.exists(r_path):\n",
    "            return f\"Missing RS file: {r_path}\"\n",
    "\n",
    "        # ---- Read climate ----\n",
    "        c_df = pd.read_csv(c_path, parse_dates=[\"date\"])\n",
    "\n",
    "        # ---- Read RS ----\n",
    "        r_df = pd.read_csv(r_path, parse_dates=[\"date\"])\n",
    "\n",
    "        # ---- Merge ----\n",
    "        df_merged = pd.merge(c_df, r_df, on=\"date\", how=\"inner\")\n",
    "\n",
    "        if df_merged.empty:\n",
    "            return f\"No data for {nuts_id}_{year}\"\n",
    "\n",
    "        # ---- Remove leap day (Feb 29) ----\n",
    "        df_merged = df_merged[\n",
    "            ~((df_merged[\"date\"].dt.month == 2) & (df_merged[\"date\"].dt.day == 29))\n",
    "        ]\n",
    "\n",
    "        # ---- Downcast float64 â†’ float32 ----\n",
    "        float_cols = df_merged.select_dtypes(include=[\"float64\"]).columns\n",
    "        df_merged[float_cols] = df_merged[float_cols].astype(\"float32\")\n",
    "\n",
    "        # ---- Save parquet ----\n",
    "        out_path = os.path.join(out_dir, f\"{nuts_id}_{year}.parquet\")\n",
    "        df_merged.to_parquet(out_path, index=False)\n",
    "\n",
    "        return f\"Saved: {nuts_id}_{year}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error in group {nuts_id}_{year}: {e}\"\n",
    "\n",
    "\n",
    "def convert_grouped_to_parquet_parallel(climate_dir, rs_dir, out_dir, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Parallel runner for process_grouped_files().\n",
    "    Expects valid_indices to be a dict-like:\n",
    "        valid_indices[(nuts_id, year)] = [grid_ids...]\n",
    "    \"\"\"\n",
    "\n",
    "    # All (NUTS_ID, year) groups to process\n",
    "    grouped_keys = list(valid_indices.keys())\n",
    "\n",
    "    print(f\"ðŸš€ Starting parallel processing on {len(grouped_keys)} NUTS-year groups...\")\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_grouped_files)(nuts_id, year, climate_dir, rs_dir, out_dir)\n",
    "        for (nuts_id, year) in tqdm(grouped_keys)\n",
    "    )\n",
    "\n",
    "    # Print summary\n",
    "    for r in results:\n",
    "        print(r)\n",
    "\n",
    "    print(\"âœ… Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f210c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run parallel conversion\n",
    "convert_grouped_to_parquet_parallel(\n",
    "    climate_dir=climate_dir, rs_dir=rs_dir, out_dir=out_dir, n_jobs=70\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f1e04",
   "metadata": {},
   "source": [
    "## Check all files has same number of rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f747e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file_paths = glob(os.path.join(out_dir, \"*.parquet\"))\n",
    "n_rows = len(pd.read_parquet(parquet_file_paths[0]))\n",
    "error_files = []\n",
    "for path in tqdm(parquet_file_paths):\n",
    "    f_name = os.path.basename(path)\n",
    "    if len(pd.read_parquet(path)) != n_rows:\n",
    "        error_files.append(f_name)\n",
    "        print(\"Error files:\", f_name, f_name.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
